[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring and visualizing meta-analytical data using R",
    "section": "",
    "text": "Welcome\nThis training module is designed to provide an introduction to data exploration and visualization using , with a particular focus on meta-analysis. The course is built around hands-on exercises aimed at developing essential skills for manipulating, summarizing, and presenting data in a clear, visually appealing format. By leveraging  packages such as ggplot2, cowplot, and rnaturalearth, participants will learn how to generate meaningful visual representations, such as histograms, maps, treemaps, heatmaps, and bubble plots.\nThe data used in this course comes from various scientific studies that analyze the impact of human interventions on various outcomes. The exercises cover a wide range of techniques, from simple bar charts to more complex visualizations like interactive maps and dynamic tables. The primary goal is to equip participants with practical tools to explore large dataset, summarize findings, and effectively communicate results in scientific contexts.\nWhether you’re new to  or looking to expand your data visualization skills, this module will provide you with a strong foundation in the fundamentals of data analysis and graphical presentation, emphasizing clarity, accuracy, and aesthetic quality in your work.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#material",
    "href": "index.html#material",
    "title": "Exploring and visualizing meta-analytical data using R",
    "section": "Material",
    "text": "Material\nAll the material used in this book is available at: https://github.com/dbeillouin/Visualisation1/",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Exploring and visualizing meta-analytical data using R",
    "section": "Citation",
    "text": "Citation\n\nAdd a citation…",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contributions",
    "href": "index.html#contributions",
    "title": "Exploring and visualizing meta-analytical data using R",
    "section": "Contributions",
    "text": "Contributions\nIf you see mistakes or want to suggest changes, please create an issue on the source repository.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#reuse",
    "href": "index.html#reuse",
    "title": "Exploring and visualizing meta-analytical data using R",
    "section": "Reuse",
    "text": "Reuse\nText and figures are licensed under Creative Commons Attribution CC By 4.0, unless otherwise noted.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "chapters/introduction.html",
    "href": "chapters/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Toward transparency and reproducibility\nThe FAIR principles (Findable, Accessible, Interoperable, and Reusable) provide a framework for improving the management and sharing of digital research assets. These principles ensure that data is discoverable through search engines, accessible with appropriate authorization, interoperable with other datasets, and reusable for various purposes. A key aspect is machine-actionability, enabling computers to process and understand data without significant human intervention.\nFrance has made significant strides in promoting open science. The French National Plan for Open Science (2021-2024) mandates open access to both scientific publications and research data generated with public funding.\nThe overarching objective of this plan is to promote transparency, accessibility, and the preservation of scientific knowledge. By mandating open access, France ensures that research funded by public resources benefits the wider global community, fostering international collaboration and cross-disciplinary advancements.\nA cornerstone of this effort is the adoption of the FAIR principles (Findable, Accessible, Interoperable, Reusable), which are integral in addressing common challenges in data management. By adhering to these principles, research data becomes more reliably reusable, supporting better documentation, accessibility, and data compatibility across different systems and disciplines.\nBreakdown of FAIR Principles:",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/introduction.html#toward-transparency-and-reproducibility",
    "href": "chapters/introduction.html#toward-transparency-and-reproducibility",
    "title": "Introduction",
    "section": "",
    "text": "Findable: Data must be easy to locate for both humans and machines. This involves assigning globally unique identifiers (such as DOIs) to datasets and ensuring that metadata is searchable and indexed in databases.\nAccessible: Data must be accessible under clear and transparent terms. This means storing datasets in repositories that guarantee long-term access, either through open-access platforms or specialized data journals that maintain the integrity of the data over time.\nInteroperable: Data should be compatible with other datasets and tools. Standardized formats (e.g., CSV, JSON) and recognized metadata structures like Dublin Core help ensure that datasets can be integrated and compared across different systems.\nReusable: Data must be well-documented, with detailed metadata providing sufficient context to allow future researchers to reuse it effectively. This includes information on the dataset’s provenance, context, and usage conditions, ensuring that it can be reliably understood and repurposed in new research contexts.\n\n\nReporting Standards in systematic reviews: PRISMA, ROSES, and Beyond\nIn systematic reviews and meta-analyses, following standardized reporting guidelines is essential for transparency and reproducibility. The most widely used framework is the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guideline, which outlines the minimum information that should be included in a systematic review, covering everything from search strategies to result synthesis. PRISMA encourages the use of flow diagrams to illustrate the study selection process, making the review process clear and replicable.\nFor environmental and social sciences, the ROSES (Reporting Standards for Systematic Evidence Syntheses) framework offers a tailored alternative. It includes checklists and flow diagrams similar to PRISMA but adapted for the specific challenges of conducting systematic reviews in complex, interdisciplinary fields like ecology, conservation, and agriculture.\nUsing these frameworks ensures:\n\nTransparency in Study Selection and Data Extraction: Flow diagrams such as the PRISMA diagram clearly document how many studies were identified, screened, and ultimately included in the synthesis. This transparency helps prevent biases in study selection and allows future researchers to see the logic behind inclusion and exclusion criteria.\nComprehensive Reporting of Methods and Results: Both PRISMA and ROSES encourage detailed reporting of the data extraction process, statistical methods used in meta-analyses, and sensitivity analyses, which are crucial for assessing the robustness of results.\nEnhanced Reproducibility: These guidelines ensure that other researchers can reproduce the review process, validate findings, and use the extracted data for new meta-analyses, secondary syntheses, or policy assessments.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/introduction.html#publishing-fully-reproducible-protocols",
    "href": "chapters/introduction.html#publishing-fully-reproducible-protocols",
    "title": "Introduction",
    "section": "Publishing Fully Reproducible Protocols",
    "text": "Publishing Fully Reproducible Protocols\nWhile pre-registration of protocols has become standard practice in fields like medicine—facilitated by platforms such as PROSPERO—it is still in the early stages of adoption within agronomy and ecology. In evidence synthesis and meta-analysis, publishing detailed and reproducible research protocols is increasingly recognized as essential for enhancing transparency and minimizing bias. This approach is well-established in medical research, where systematic reviews and meta-analyses typically adhere to stringent pre-registration guidelines. However, it has yet to gain similar traction in agronomy and ecology, highlighting an important area for growth and improvement in these disciplines. Encouraging the use of pre-published protocols in these fields would improve methodological rigor, comparability of results, and overall transparency in environmental and agricultural research. Protocols describe the step-by-step methodologies researchers intend to follow before conducting a study. They ensure transparency, reproducibility, and consistency in systematic reviews, meta-analyses, and other research designs by pre-registering the research questions, criteria for study inclusion, and planned analytical methods. This practice minimizes bias, prevents selective reporting, and enhances the credibility of findings.\n\nKey Components of a Research Protocol\n\nResearch Objectives and Questions: Clearly defines the goals of the study and the specific research questions to be addressed.\nEligibility Criteria: Specifies which studies will be included or excluded based on predefined parameters (e.g., study design, population characteristics, intervention type).\nSearch Strategy: Describes the databases, search terms, and timeframe for literature searches.\nData Extraction and Coding: Outlines the methods for extracting, coding, and managing data, including variable definitions and metadata structures.\nRisk of Bias and Quality Assessment: Details the criteria and tools used to assess the quality and potential biases of included studies.\nAnalytical Plan: Pre-specifies statistical methods, models, and subgroup analyses to be used, ensuring that analytical choices are not influenced by observed results.\n\n\n\nImportance in Meta-Analyses and Evidence Synthesis\nPublishing a detailed protocol before initiating a meta-analysis or systematic review is crucial for avoiding bias and maintaining scientific rigor. Protocols act as a roadmap, guiding researchers through the review process and serving as a reference point against which deviations can be assessed. This is particularly important for high-stakes reviews, such as those informing policy decisions or large-scale evidence syntheses in public health and environmental sciences.\nWell-developed protocols also enhance collaboration and standardization within research communities by enabling other researchers to replicate or build upon the same methodology. In ecological and agronomic meta-analyses, where diverse study designs and heterogeneous data sources are common, robust protocols are indispensable for harmonizing evidence and ensuring comparability across studies.\n\n\nStandards and Guidelines\nSeveral frameworks provide comprehensive guidance for developing and publishing reproducible protocols:\n\nCochrane Handbook for Systematic Reviews: The Cochrane Collaboration sets the gold standard for systematic reviews in health and medical research. Its protocols follow a highly structured format that emphasizes transparency, replicability, and methodological rigor.\nROSES (RepOrting standards for Systematic Evidence Syntheses): Tailored for ecological and environmental sciences, the ROSES framework outlines specific guidelines for planning and reporting systematic reviews and maps in these fields.\nPRISMA-P (Preferred Reporting Items for Systematic Review and Meta-Analysis Protocols): PRISMA-P is designed to standardize the reporting of protocols for systematic reviews and meta-analyses, ensuring all critical elements are included.\n\n\n\nJournals Specializing in Protocols\nSeveral specialized journals focus on publishing research protocols, providing a platform for researchers to share detailed methodological plans and facilitate reproducibility:\n\nBMC Systematic Reviews: Publishes protocols and reviews in health, social, and environmental sciences. BMC Systematic Reviews requires that all protocols adhere to PRISMA-P or similar reporting standards.\nProtocols.io: An open-access platform that allows researchers to publish detailed experimental protocols, workflows, and analysis pipelines. It is widely used across disciplines to promote transparent research.\nBMJ Open: Features protocols for any research area, including environmental, health, and social sciences. The journal emphasizes open science and reproducibility.\nNature Protocols: Focuses on detailed experimental protocols in life sciences. Although primarily designed for laboratory research, it offers high visibility for methodological papers.\nPROSPERO: An international database for pre-registering protocols of systematic reviews focused on health and social care.\n\n\n\nExample of a Protocol Publication\nRousset, C., Segura, C., Gilgen, A., Alfaro, M., Mendes, L.A., Dodd, M., Dashpurev, B., Bastidas, M., Rivera, J., Merbold, L. and Vázquez, E., 2024. What evidence exists relating the impact of different grassland management practices to soil carbon in livestock systems? A systematic map protocol. Environmental Evidence, 13(1), p.22.\nThis protocol describes a systematic review and meta-analysis aimed at adapting health systems in crisis settings. The document pre-specifies all methodological details, including eligibility criteria, data extraction strategies, and planned analyses, ensuring reproducibility and transparency throughout the study.\n\n\nUseful Links for Protocol Standards and Templates\n\nCochrane Handbook for Systematic Reviews of Interventions: Cochrane Handbook\nPRISMA-P Reporting Guidelines: PRISMA-P Checklist\nROSES Guidelines for Environmental Sciences: ROSES Reporting Standards\nEquator Network: A comprehensive resource for research reporting guidelines and protocol standards: Equator Network\n\nuseful links: https://environmentalevidencejournal.biomedcentral.com/submission-guidelines/preparing-your-manuscript/systematic-review-protocol",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/introduction.html#publish-a-datapaper",
    "href": "chapters/introduction.html#publish-a-datapaper",
    "title": "Introduction",
    "section": "Publish a DataPaper",
    "text": "Publish a DataPaper\nA Data Paper is a publication dedicated to describing the structure, collection, and value of a dataset. Unlike traditional research papers, which focus on findings and interpretations, Data Papers emphasize the metadata, methodology, and potential uses of the dataset itself. They offer detailed insights into how the data was gathered, processed, and structured, which is essential for reproducibility in scientific studies. Key Components of a Data Paper:\n\nDataset Overview: Provides a summary of the dataset, including its purpose and potential applications.\n\nMetadata: Describes each variable, including units of measurement, data types, and any transformations applied.\n\nCollection Methods: Details the experimental or observational methods used to gather the data.\n\nLimitations and Uncertainties: Discloses any potential biases, gaps, or limitations in the dataset.\n\nData Access: Specifies how the data can be accessed and reused, often with a permanent DOI link.\n\nIn evidence mapping and meta-analyses, the publication of Data Papers ensures that large datasets, which could be difficult to interpret otherwise, are accompanied by clear, accessible documentation. This reduces barriers to data reuse and promotes collaboration across research communities.\nJournals Publishing Data Papers\nSeveral specialized journals focus on publishing Data Papers, promoting high-quality data curation and sharing. Scientific Data(by Nature Research) and Data in Brief (by Elsevier) are prominent examples, offering platforms for data-specific publications. These journals often require the dataset to be archived in an open-access repository, accompanied by rich metadata, and adhere to rigorous peer review processes. For example, Biodiversity Data Journal also publish data papers focused on biodiversity and ecological datasets. This ensures that the data shared is of high quality, reusable, and follows best practices for transparency and openness.\nExample of Data Papers:\n\nBeillouin, Damien, Marc Corbeels, Julien Demenois, David Berre, Annie Boyer, Abigail Fallot, Frédéric Feder, and Rémi Cardinael. “A global meta-analysis of soil organic carbon in the Anthropocene.” Nature Communications 14, no. 1 (2023): 3700.\nByun, E., Müller, C., Parisse, B., Napoli, R., Zhang, J.B., Rezanezhad, F., Van Cappellen, P., Moser, G., Jansen-Willems, A.B., Yang, W.H. and Urakawa, R., 2024. A global dataset of gross nitrogen transformation rates across terrestrial ecosystems. Scientific Data, 11(1), p.1022.\n\nuseful links: \n-   CIRAD publier un Datapaper https://coop-ist.cirad.fr/gerer-des-donnees/publier-un-data-paper/1-qu-est-ce-qu-un-data-paper \n\n-   CINES, 2017. Les formats de fichier. https://www.cines.fr/archivage/des-expertises/les-formats-de-fichier/\n\n-   CNRS, 2023 (version 2.0) . Guide de bonnes pratiques sur la gestion des données de recherche. Publier un Datapaper pour valoriser et expliciter les données. https://mi-gt-donnees.pages.math.unistra.fr/guide/00-introduction.html\n\n-   DoRANum, 2018. La minute Publier un Data paper. https://doi.org/10.13143/4mhn-mq42\n\nPublishing in Open Access Journals\nWhen submitting research for publication, consider choosing open access journals, particularly those that operate on a non-profit basis. This approach ensures that publicly funded research is readily accessible to the public, promoting transparency and facilitating broader dissemination of knowledge. Open access publishing removes paywalls, allowing researchers, practitioners, and policymakers to engage with your work without financial barriers, thereby enhancing the impact and reach of your findings. Prioritizing non-profit journals also supports sustainable publishing practices that align with the principles of open science.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/introduction.html#best-practices-for-structuring-meta-analysis-datafiles",
    "href": "chapters/introduction.html#best-practices-for-structuring-meta-analysis-datafiles",
    "title": "Introduction",
    "section": "Best practices for structuring Meta-analysis DataFiles",
    "text": "Best practices for structuring Meta-analysis DataFiles\n\nGeneralities\n\nConsistent Naming Conventions: Ensure that file names are clear, consistent, and meaningful. For example, naming columns such as Study_ID, Outcome, Intervention, and Effect_Size helps in avoiding confusion during data manipulation. Avoid special characters in column names, and use underscores or camel case for readability (e.g., StudyName or study_name).\nComprehensive Metadata: Metadata should accompany the main data file, providing explanations of each column and the coding used (e.g., what constitutes “intervention type” or “effect size unit”). A “Data Dictionary” should always be part of your dataset, explaining variables such as:\n\nOutcome: The primary outcome measured in the study.\nIntervention: Types of interventions, such as “land-use change” or “management.”\nEffect_Size: Numeric or categorical data on effect size (e.g., Hedge’s g or Cohen’s d).\n\nWide vs. Long Format: Choose the format that best suits your analysis:\n\n\nWide Format: Used when each row represents a study, with multiple columns for each outcome (e.g., separate columns for effect sizes).\n\n\n\nField\nSoil pH\nNitrogen Content (%)\nCrop Yield (kg/ha)\n\n\n\n\n1\n6.5\n45\n3000\n\n\n2\n6.8\n50\n3200\n\n\n3\n6.2\n40\n2800\n\n\n\nLong Format: More suitable for meta-analysis and visualization in R. Each row contains a single observation or a study’s outcome, which allows for easier aggregation, filtering, and plotting.\n\n\n\n\nField\nVariable\nValue\n\n\n\n\n1\nSoil pH\n6.5\n\n\n1\nNitrogen Content\n45\n\n\n1\nCrop Yield\n3000\n\n\n2\nSoil pH\n6.8\n\n\n2\nNitrogen Content\n50\n\n\n2\nCrop Yield\n3200\n\n\n3\nSoil pH\n6.2\n\n\n3\nNitrogen Content\n40\n\n\n3\nCrop Yield\n2800\n\n\n\n\nHandling Missing Data: It’s common to encounter missing data in meta-analyses. Best practices include:\n\nUsing a consistent code for missing values, such as NA.\nAvoiding empty cells, which can cause issues when importing data into R.\nDocumenting missing data in the metadata.\n\nVersion Control: Ensure version control for your datasets. Tools like Git or a simple versioning system (e.g., dataset_v1.csv, dataset_v2.csv) can help track changes and maintain the integrity of your data over time.\nData Cleanliness: Ensure all numeric data are formatted correctly (e.g., avoid mixing numbers and text in the same column). Double-check for typographical errors, duplicates, and inconsistencies in categorical data. Tools like dplyr::mutate() and tidyr::pivot_longer() can aid in cleaning and restructuring data for analysis.\n\n\n\nharmonisable classifications of practices and outcome\nMeta-analysis and evidence synthesis necessitate consistent and harmonized classifications of interventions, practices, and outcomes to ensure the comparability of findings across studies and geographic contexts. In agricultural and ecological research, the diversity of practices, variations in terminology, and the complex relationships between interventions and their impacts on multiple outcomes complicate this classification task. This chapter highlights the importance of employing ontologies as a foundational step in developing harmonizable classifications. Investing the time to establish clear definitions and boundaries between classes for practices, outcomes, and site descriptions is crucial. A well-defined research question can further refine the scope, facilitating the classification process. By systematically categorizing agricultural practices and outcomes, researchers can enhance the rigor and relevance of meta-analytical studies, ultimately contributing to more robust evidence synthesis",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/introduction.html#example-meta-analysis-datasets",
    "href": "chapters/introduction.html#example-meta-analysis-datasets",
    "title": "Introduction",
    "section": "Example: Meta-analysis datasets",
    "text": "Example: Meta-analysis datasets\nTo explore and utilize meta-analysis datasets, you can refer to the metadat package in R, which provides a comprehensive collection of datasets tailored for teaching, illustrating meta-analytic methods, and validating published analyses. You can install the package from CRAN using:\nOnce installed, you can browse available datasets by using:\n\n# install metadat package\n#install.packages(\"metadat\")\n\n# load metadat package\nlibrary(metadat)\n\n#List of dataset included\nhelp(package = metadat)\n\nEach dataset is well-documented with metadata, including concept terms such as research field, outcome measures, and analytic models. These metadata provide insight into the structure and purpose of each dataset. Additionally, the datsearch() function allows you to search for datasets based on specific concept terms or perform a full-text search through their documentation.\nThe datasets in metadat follow structured formats, typically containing variables related to effect sizes, moderators, and sample information. To contribute or explore more in-depth examples, visit the package’s online documentation at metadat GitHub, where you can also view the output of example analyses for each dataset\n\n# load curtis databse\ndat &lt;- dat.curtis1998\n\n# Explore curtis data\n#install.packages(\"skimr\")\nlibrary(skimr)\nhead(dat)\n\n  id paper   genus     species fungrp co2.ambi co2.elev units time pot method\n1 21    44   ALNUS       RUBRA  N2FIX      350      650  ul/l   47 0.5     GC\n2 22    44   ALNUS       RUBRA  N2FIX      350      650  ul/l   47 0.5     GC\n3 27   121    ACER      RUBRUM  ANGIO      350      700   ppm   59 2.6     GH\n4 32   121 QUERCUS      PRINUS  ANGIO      350      700   ppm   70 2.6     GH\n5 35   121   MALUS   DOMESTICA  ANGIO      350      700   ppm   64 2.6     GH\n6 38   121    ACER SACCHARINUM  ANGIO      350      700   ppm   50 2.6     GH\n  stock xtrt   level     m1i      sd1i n1i    m2i      sd2i n2i\n1  SEED FERT    HIGH  6.8169 1.7699820   3 3.9450 1.1157970   5\n2  SEED FERT CONTROL  2.5961 0.6674662   5 2.2512 0.3275839   5\n3  SEED NONE       .  2.9900 0.8560000   5 1.9300 0.5520000   5\n4  SEED NONE       .  5.9100 1.7420000   5 6.6200 1.6310000   5\n5  SEED NONE       .  4.6100 1.4070000   4 4.1000 1.2570000   4\n6  SEED NONE       . 10.7800 1.1630000   5 6.4200 2.0260000   3\n\n\n\nDataset for Our Exercises\nWe will be using the dataset titled A Global Database of Diversified Farming Effects on Biodiversity and Yield. This comprehensive dataset includes 4,076 comparisons of biodiversity outcomes and 1,214 comparisons of yield in diversified farming systems, contrasting these outcomes with two reference systems.\nThe dataset encompasses evidence from 48 countries and evaluates the effects of diversified farming systems on species across 33 taxonomic orders, including insects, plants, birds, mammals, eukaryotes, annelids, fungi, and bacteria. It specifically addresses systems that produce both annual and perennial crops across 12 commodity groups.\nThis dataset serves as a valuable resource for researchers and practitioners, facilitating access to critical information regarding the positive contributions of diversified farming systems to both biodiversity and food production outcomes.\n\n\nSteps to Access the Dataset\n\nLoad the File from Harvard Dataverse\nVisit the Harvard Dataverse website.\nDownload all files and save them in your current working directory.\n\nBy following these steps, you will be well-equipped to utilize the dataset for the upcoming exercises in this module.\n\n\n# install.packages(\"readxl\")\n# charge le fichier\nMeta_Data &lt;- readxl::read_excel(path  = here::here(\"data\", \"dataset-1-sources.xlsx\"), \n                                sheet = \"Literature_screened\")\nhead(Meta_Data)\n\n# A tibble: 6 × 19\n     ID Article_source  Inclusion_yes_no Exclusion_reasion_pico Exclusion_reason\n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;                  &lt;chr&gt;           \n1    13 Stakeholder re… Yes              NA                     NA              \n2    15 Scopus or WoS   No               Unsuitable outcomes    Effect on yield…\n3    18 Stakeholder re… Yes              NA                     NA              \n4    25 Scopus or WoS   No               Unsuitable population  Irrelevant      \n5    30 Stakeholder re… Yes              NA                     NA              \n6    34 Stakeholder re… Yes              NA                     NA              \n# ℹ 14 more variables: Authors &lt;chr&gt;, Year &lt;dbl&gt;, Title &lt;chr&gt;,\n#   Source.title &lt;chr&gt;, Volume &lt;chr&gt;, Issue &lt;chr&gt;, Art_No &lt;chr&gt;,\n#   Page_start &lt;chr&gt;, Page_end &lt;chr&gt;, Page_count &lt;chr&gt;, DOI &lt;chr&gt;, Link &lt;chr&gt;,\n#   ISSN &lt;chr&gt;, ISBN &lt;chr&gt;\n\nOutcome &lt;- readxl::read_excel(path  = here::here(\"data\", \"dataset-2-outcomes.xlsx\"), \n                              sheet = \"Data\")\nhead(Outcome)\n\n# A tibble: 6 × 105\n     ID Experiment_stage Comparison_ID_C Comparison_class_C Crop_C Crop_FAO_C   \n  &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;           &lt;chr&gt;              &lt;chr&gt;  &lt;chr&gt;        \n1  1033 1                C1              Natural            Forest NA           \n2  1033 1                C2              Simplified         Coffee 12 - STIMULA…\n3  1033 2                C1              Natural            Forest NA           \n4  1033 2                C2              Simplified         Coffee 12 - STIMULA…\n5  1033 3                C2              Simplified         Coffee 12 - STIMULA…\n6  1033 4                C1              Natural            Forest NA           \n# ℹ 99 more variables: Crop_ann_pen_C &lt;chr&gt;, Crop_woodiness_C &lt;chr&gt;,\n#   crops_all_common_C &lt;chr&gt;, crops_all_scientific_C &lt;chr&gt;,\n#   crops_all_scientific_level_C &lt;chr&gt;, System_raw_C &lt;chr&gt;,\n#   System_details_C &lt;chr&gt;, System_C &lt;chr&gt;, Fertiliser_C &lt;chr&gt;,\n#   Fertiliser_chem_C &lt;chr&gt;, Pesticide_C &lt;chr&gt;, Pesticide_quantity_C &lt;chr&gt;,\n#   Soil_management_C &lt;chr&gt;, Time_state_C &lt;chr&gt;, Study_length_C &lt;chr&gt;,\n#   Sampling_unit_C &lt;chr&gt;, B_error_measure_C &lt;chr&gt;, B_error_value_C &lt;dbl&gt;, …",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/temporal-analyses.html",
    "href": "chapters/temporal-analyses.html",
    "title": "Temporal analysis",
    "section": "",
    "text": "Recommendations\nWhen visualizing temporal data, the choice of graph type is crucial for accurately conveying trends:\nIn some cases, a combination of both graph types can provide a comprehensive view of temporal trends, illustrating both cumulative growth and specific spikes in research output.",
    "crumbs": [
      "Temporal analysis"
    ]
  },
  {
    "objectID": "chapters/temporal-analyses.html#recommendations",
    "href": "chapters/temporal-analyses.html#recommendations",
    "title": "Temporal analysis",
    "section": "",
    "text": "Cumulative Sum Graphs: These graphs represent the total number of publications over time, allowing researchers to visualize the overall growth of knowledge in a field. They can illustrate how research attention has increased, revealing long-term trends and shifts in focus.\nCount Graphs: These graphs show the number of publications per time period (e.g., year), enabling the identification of specific periods of increased research activity. They can highlight trends that may warrant further investigation or indicate reactions to external events.",
    "crumbs": [
      "Temporal analysis"
    ]
  },
  {
    "objectID": "chapters/temporal-analyses.html#practice",
    "href": "chapters/temporal-analyses.html#practice",
    "title": "Temporal analysis",
    "section": "Practice",
    "text": "Practice\nTo illustrate temporal analysis in R, we will use the ggplot2 and dplyr packages to create both cumulative sum and count graphs. Below is an example of how to plot these two types of graphs using fictional publication data.\n\nPreamble\nLoading packages\n\n# Load required libraries ----\nlibrary(ggplot2)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nImporting dataset\n\nmetadata &lt;- readxl::read_excel(path  = here::here(\"data\", \"dataset-1-sources.xlsx\"), \n                               sheet = \"Literature_screened\")\n\noutcome &lt;- readxl::read_excel(path  = here::here(\"data\", \"dataset-2-outcomes.xlsx\"), \n                              sheet = \"Data\")\n\n\n# Merge the outcome dataset with metadata\n# TAB should be your combined dataset, with relevant variables\nTAB &lt;- left_join(outcome, metadata)\n\n\n\nCount Graph\nThis graph visualizes the count of different studies published each year. It uses a bar graph to display the number of studies per publication year, making it easy to compare across years\n\n# Count Graph\ncount_data &lt;- TAB %&gt;%\n  group_by(Year) %&gt;%  # Group data by publication year\n  summarise(Count = n(), .groups = \"drop\")  # Count studies per year\n\n# Plot the count graph\nggplot(count_data, aes(x = Year, y = Count)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +  # Create a bar graph\n  labs(title = \"Number of Studies Published per Year\",\n       x     = \"Publication Year\",\n       y     = \"Number of Studies\") +\n  theme_minimal()  # Use a minimal theme for clarity\n\n\n\n\n\n\n\n\n\n\nCumulative Sum Graph\nThis graph displays the cumulative count of studies published over the years. It helps visualize how the total number of studies increases as time progresses.\n\n# Cumulative Sum Graph\ncumulative_data &lt;- count_data %&gt;%\n  arrange(Year) %&gt;%  # Ensure data is sorted by year\n  mutate(Cumulative = cumsum(Count))  # Calculate cumulative count\n\n# Plot the cumulative sum graph\nggplot(cumulative_data, aes(x = Year, y = Cumulative)) +\n  geom_line(linewidth = 1, color = \"darkgreen\") +  # Create a line graph\n  labs(title = \"Cumulative Count of Studies Published\",\n       x     = \"Year\",\n       y     = \"Cumulative Count of Studies\") +\n  theme_minimal()  # Use a minimal theme for clarity\n\n\n\n\n\n\n\n\n\n\nGrouped Graph by categories\nThis graph shows the cumulative count of studies based on different crop types. It allows for comparison of study trends across various crop systems.\n\n# Grouped Graph by Crop\ngrouped_data &lt;- TAB %&gt;%\n  group_by(Year, System_C) %&gt;% \n  summarise(Count = n(),.groups = \"drop\") %&gt;%\n  arrange(Year) %&gt;%\n  group_by(System_C) %&gt;%\n  mutate(Cumulative = cumsum(Count)) %&gt;% \n  ungroup()\n\n# Plot the grouped cumulative count graph\n\nggplot(grouped_data, aes(x     = Year, \n                         y     = Cumulative, \n                         color = System_C, \n                         group = System_C)) +\n  geom_line(linewidth = 1) + \n  geom_point(size = 2) + \n  labs(title = \"Cumulative Count of Studies per Control Type\", \n       x     = \"Year\", \n       y     = \"Cumulative Count of Studies\") +\n  theme_minimal()",
    "crumbs": [
      "Temporal analysis"
    ]
  },
  {
    "objectID": "chapters/temporal-analyses.html#brief-note-on-cumulative-meta-analyses",
    "href": "chapters/temporal-analyses.html#brief-note-on-cumulative-meta-analyses",
    "title": "Temporal analysis",
    "section": "Brief note on cumulative meta-analyses",
    "text": "Brief note on cumulative meta-analyses\nCumulative meta-analyses are another important aspect of temporal analysis. They allow researchers to assess how the effect sizes of interventions or phenomena change as new studies are added over time. This approach can provide valuable insights into the robustness of findings and help track the evolution of evidence on specific topics. Cumulative meta-analyses can further enrich evidence maps by providing a more nuanced understanding of how knowledge accumulates and shifts within a research domain.\nBy integrating temporal analysis into evidence maps and meta-analyses, researchers can enhance the depth and relevance of their findings, ultimately contributing to a more robust and informed understanding of research trends and their implications.",
    "crumbs": [
      "Temporal analysis"
    ]
  },
  {
    "objectID": "chapters/resources.html",
    "href": "chapters/resources.html",
    "title": "Resources",
    "section": "",
    "text": "Articles\n…\n\n\nBooks\n…",
    "crumbs": [
      "Resources"
    ]
  }
]